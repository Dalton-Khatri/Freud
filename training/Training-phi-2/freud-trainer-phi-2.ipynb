{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":14633699,"sourceType":"datasetVersion","datasetId":9347778}],"dockerImageVersionId":31259,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Keep session alive\nimport time\nfrom datetime import datetime\n\ndef keep_alive():\n    \"\"\"Print timestamp every 30 minutes to keep session active\"\"\"\n    while True:\n        time.sleep(1800)  # 30 minutes\n        print(f\"Keep-alive: {datetime.now().strftime('%H:%M:%S')}\")\n\n# Start keep-alive in background\nimport threading\nthread = threading.Thread(target=keep_alive, daemon=True)\nthread.start()\nprint(\"Keep-alive started!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-04T10:59:13.858947Z","iopub.execute_input":"2026-02-04T10:59:13.859121Z","iopub.status.idle":"2026-02-04T10:59:13.867938Z","shell.execute_reply.started":"2026-02-04T10:59:13.859102Z","shell.execute_reply":"2026-02-04T10:59:13.867271Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Step 1: Install Dependencies","metadata":{}},{"cell_type":"code","source":"%%capture\n!pip uninstall -y diffusers bitsandbytes\n!pip install -q torch==2.1.2\n!pip install -q transformers==4.37.2\n!pip install -q datasets==2.16.1\n!pip install -q accelerate==0.27.0\n!pip install -q peft==0.9.0\n!pip install -q trl==0.8.1\n\nprint(\"Packages installed!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-04T10:59:17.380313Z","iopub.execute_input":"2026-02-04T10:59:17.380597Z","iopub.status.idle":"2026-02-04T10:59:52.015217Z","shell.execute_reply.started":"2026-02-04T10:59:17.380573Z","shell.execute_reply":"2026-02-04T10:59:52.014254Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Step 2: Import Libraries","metadata":{}},{"cell_type":"code","source":"import torch\nimport json\nimport os\nimport gc\nfrom pathlib import Path\nfrom datetime import datetime\n\nfrom datasets import Dataset\nfrom transformers import (\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    TrainingArguments,\n    DataCollatorForLanguageModeling,\n)\nfrom peft import (\n    LoraConfig,\n    get_peft_model,\n    PeftModel,\n)\nfrom trl import SFTTrainer\n\nprint(f\"PyTorch: {torch.__version__}\")\nprint(f\"CUDA: {torch.cuda.is_available()}\")\nif torch.cuda.is_available():\n    print(f\" GPU: {torch.cuda.get_device_name(0)}\")\n    print(f\"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-04T11:00:29.313809Z","iopub.execute_input":"2026-02-04T11:00:29.314735Z","iopub.status.idle":"2026-02-04T11:00:52.590644Z","shell.execute_reply.started":"2026-02-04T11:00:29.314698Z","shell.execute_reply":"2026-02-04T11:00:52.589966Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Step 3: Configuration","metadata":{}},{"cell_type":"code","source":"print(\"Configuration\\n\")\n\n# Paths\nTRAIN_DATA_PATH = \"/kaggle/input/freud-2-0/freud_training_data/train.json\"\nVAL_DATA_PATH = \"/kaggle/input/freud-2-0/freud_training_data/validation.json\"\nOUTPUT_DIR = \"./freud_phi2_finetuned\"\nHF_MODEL_NAME = \"Dalton-Khatri/freud-phi2\"\n\n# Model\nBASE_MODEL = \"microsoft/phi-2\"\n\n# Training (optimized for P100 + FP32)\nNUM_EPOCHS = 3\nBATCH_SIZE = 1  # Small due to FP32\nGRADIENT_ACCUMULATION_STEPS = 8\nLEARNING_RATE = 2e-4\nWARMUP_RATIO = 0.03\nMAX_SEQ_LENGTH = 512\n\n# LoRA\nLORA_R = 8\nLORA_ALPHA = 16\nLORA_DROPOUT = 0.05\n\n# Checkpointing\nLOGGING_STEPS = 50\nSAVE_STEPS = 500\n\nprint(f\"Config loaded\")\nprint(f\"Effective batch: {BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS}\")\nprint(f\"Max sequence: {MAX_SEQ_LENGTH}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-04T11:01:11.345018Z","iopub.execute_input":"2026-02-04T11:01:11.345878Z","iopub.status.idle":"2026-02-04T11:01:11.351390Z","shell.execute_reply.started":"2026-02-04T11:01:11.345847Z","shell.execute_reply":"2026-02-04T11:01:11.350728Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Step 4: Load Training Data","metadata":{}},{"cell_type":"code","source":"print(\"Loading data...\\n\")\n\nwith open(TRAIN_DATA_PATH, 'r') as f:\n    train_data = json.load(f)\n\nwith open(VAL_DATA_PATH, 'r') as f:\n    val_data = json.load(f)\n\ntrain_dataset = Dataset.from_list(train_data)\nval_dataset = Dataset.from_list(val_data)\n\nprint(f\"Train: {len(train_dataset):,} samples\")\nprint(f\"Val: {len(val_dataset):,} samples\\n\")\n\nprint(\" Sample:\")\nprint(\"=\"*80)\nprint(train_dataset[0]['text'][:400])\nprint(\"=\"*80)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-04T11:01:14.922209Z","iopub.execute_input":"2026-02-04T11:01:14.923034Z","iopub.status.idle":"2026-02-04T11:01:15.156365Z","shell.execute_reply.started":"2026-02-04T11:01:14.923003Z","shell.execute_reply":"2026-02-04T11:01:15.155721Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Step 5: Load Model in FP32","metadata":{}},{"cell_type":"code","source":"print(f\"Loading {BASE_MODEL} in FP32...\\n\")\n\n# Load in FULL PRECISION (FP32)\nmodel = AutoModelForCausalLM.from_pretrained(\n    BASE_MODEL,\n    torch_dtype=torch.float32,  \n    device_map=\"auto\",\n    trust_remote_code=True,\n)\n\nprint(\"Model loaded!\")\nprint(f\"Parameters: {model.num_parameters():,}\")\nprint(f\"Data type: {next(model.parameters()).dtype}\")\nprint(f\"Device: {next(model.parameters()).device}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-04T11:01:17.594090Z","iopub.execute_input":"2026-02-04T11:01:17.594756Z","iopub.status.idle":"2026-02-04T11:01:58.866941Z","shell.execute_reply.started":"2026-02-04T11:01:17.594728Z","shell.execute_reply":"2026-02-04T11:01:58.866154Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Step 6: Load Tokenizer","metadata":{}},{"cell_type":"code","source":"print(\" Loading tokenizer...\\n\")\n\ntokenizer = AutoTokenizer.from_pretrained(\n    BASE_MODEL,\n    trust_remote_code=True,\n)\n\n# Set padding token\nif tokenizer.pad_token is None:\n    tokenizer.pad_token = tokenizer.eos_token\n    model.config.pad_token_id = tokenizer.eos_token_id\n\nprint(\" Tokenizer ready!\")\nprint(f\"Vocab: {len(tokenizer):,}\")\nprint(f\"EOS: {tokenizer.eos_token} ({tokenizer.eos_token_id})\")\nprint(f\"PAD: {tokenizer.pad_token} ({tokenizer.pad_token_id})\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-04T11:02:03.782211Z","iopub.execute_input":"2026-02-04T11:02:03.782890Z","iopub.status.idle":"2026-02-04T11:02:05.297959Z","shell.execute_reply.started":"2026-02-04T11:02:03.782843Z","shell.execute_reply":"2026-02-04T11:02:05.297246Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Step 7: Apply LoRA","metadata":{}},{"cell_type":"code","source":"print(\"Applying LoRA...\\n\")\n# Enable gradient checkpointing\nmodel.gradient_checkpointing_enable()\n\nlora_config = LoraConfig(\n    r=LORA_R,\n    lora_alpha=LORA_ALPHA,\n    lora_dropout=LORA_DROPOUT,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\",\n    target_modules=[\"Wqkv\", \"fc1\", \"fc2\"],  \n)\n\nmodel = get_peft_model(model, lora_config)\n\ntrainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\ntotal = sum(p.numel() for p in model.parameters())\n\nprint(\"LoRA applied!\")\nprint(f\"Trainable: {trainable:,} ({100*trainable/total:.2f}%)\")\nprint(f\"Total: {total:,}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-04T11:02:30.187545Z","iopub.execute_input":"2026-02-04T11:02:30.188070Z","iopub.status.idle":"2026-02-04T11:02:30.348957Z","shell.execute_reply.started":"2026-02-04T11:02:30.188041Z","shell.execute_reply":"2026-02-04T11:02:30.348202Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Step 8: Configure Training Arguments","metadata":{}},{"cell_type":"code","source":"print(\"Setting up training...\\n\")\n\ntraining_args = TrainingArguments(\n    output_dir=OUTPUT_DIR,\n    num_train_epochs=NUM_EPOCHS,\n    per_device_train_batch_size=BATCH_SIZE,\n    per_device_eval_batch_size=BATCH_SIZE,\n    gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n    learning_rate=LEARNING_RATE,\n    warmup_ratio=WARMUP_RATIO,\n    \n    fp16=False,  \n    bf16=False,  \n    \n    # Optimizer\n    optim=\"adamw_torch\",\n    weight_decay=0.01,\n    max_grad_norm=1.0,\n    \n    # Logging\n    logging_steps=LOGGING_STEPS,\n    logging_dir=f\"{OUTPUT_DIR}/logs\",\n    \n    # Saving\n    save_strategy=\"steps\",\n    save_steps=SAVE_STEPS,\n    save_total_limit=2,\n    \n    # Evaluation\n    evaluation_strategy=\"steps\",\n    eval_steps=SAVE_STEPS,\n    load_best_model_at_end=True,\n    metric_for_best_model=\"loss\",\n    greater_is_better=False,\n    \n    # Misc\n    report_to=\"none\",\n    dataloader_num_workers=0,\n    remove_unused_columns=False,\n    seed=42,\n)\n\nprint(\"Training args set!\")\nprint(f\"\\n Settings:\")\nprint(f\"   - Precision: FP32 (no mixed precision)\")\nprint(f\"   - Batch: {BATCH_SIZE} x {GRADIENT_ACCUMULATION_STEPS} = {BATCH_SIZE*GRADIENT_ACCUMULATION_STEPS}\")\nprint(f\"   - Learning rate: {LEARNING_RATE}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-04T11:02:33.904046Z","iopub.execute_input":"2026-02-04T11:02:33.904443Z","iopub.status.idle":"2026-02-04T11:02:33.912546Z","shell.execute_reply.started":"2026-02-04T11:02:33.904413Z","shell.execute_reply":"2026-02-04T11:02:33.911884Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Step 9: Create SFTTrainer","metadata":{}},{"cell_type":"code","source":"print(\" Creating trainer...\\n\")\n\n# Set environment variable\nimport os\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n\n# Clear GPU cache\ngc.collect()\ntorch.cuda.empty_cache()\n\nprint(f\"ðŸ’¾ GPU allocated: {torch.cuda.memory_allocated()/1024**3:.2f} GB\")\nprint(f\"ðŸ’¾ GPU reserved: {torch.cuda.memory_reserved()/1024**3:.2f} GB\")\n\ndef tokenize_function(examples):\n    \"\"\"Tokenize the text field\"\"\"\n    return tokenizer(\n        examples[\"text\"],\n        truncation=True,\n        max_length=MAX_SEQ_LENGTH,\n        padding=False, \n    )\n\nprint(\"Tokenizing datasets...\")\ntokenized_train = train_dataset.map(\n    tokenize_function,\n    batched=True,\n    remove_columns=train_dataset.column_names,\n    desc=\"Tokenizing train\",\n)\ntokenized_val = val_dataset.map(\n    tokenize_function,\n    batched=True,\n    remove_columns=val_dataset.column_names,\n    desc=\"Tokenizing validation\",\n)\nprint(\"Tokenization complete!\\n\")\n\n# Create data collator\nfrom transformers import DataCollatorForLanguageModeling\ndata_collator = DataCollatorForLanguageModeling(\n    tokenizer=tokenizer,\n    mlm=False,  \n)\n\nfrom transformers import Trainer \ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_train,\n    eval_dataset=tokenized_val,\n    data_collator=data_collator,\n)\n\nprint(\"Trainer ready!\\n\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-04T11:02:37.016305Z","iopub.execute_input":"2026-02-04T11:02:37.016856Z","iopub.status.idle":"2026-02-04T11:02:40.517511Z","shell.execute_reply.started":"2026-02-04T11:02:37.016825Z","shell.execute_reply":"2026-02-04T11:02:40.516867Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Step 10: Start Training ðŸš€","metadata":{}},{"cell_type":"code","source":"print(\"=\"*80)\nprint(\"STARTING TRAINING\")\nprint(\"=\"*80)\nprint(f\"Started: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\nprint(\"\\nExpected time: ~4-5 hours\")\nprint(\"Checkpoints every 500 steps\\n\")\n\n# Train!\ntrain_result = trainer.train()\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"TRAINING COMPLETE!\")\nprint(\"=\"*80)\nprint(f\"Finished: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\nprint(f\"\\nFinal loss: {train_result.training_loss:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-04T11:02:42.969143Z","iopub.execute_input":"2026-02-04T11:02:42.969449Z","iopub.status.idle":"2026-02-04T16:03:48.326417Z","shell.execute_reply.started":"2026-02-04T11:02:42.969425Z","shell.execute_reply":"2026-02-04T16:03:48.325752Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Step 11: Save Model","metadata":{}},{"cell_type":"code","source":"print(\"\\nSaving model...\\n\")\n\ntrainer.save_model(OUTPUT_DIR)\ntokenizer.save_pretrained(OUTPUT_DIR)\n\nprint(f\"Saved to: {OUTPUT_DIR}/\")\nprint(\"\\nFiles:\")\nfor f in Path(OUTPUT_DIR).glob(\"*\"):\n    print(f\"   - {f.name}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-04T16:04:40.673081Z","iopub.execute_input":"2026-02-04T16:04:40.674010Z","iopub.status.idle":"2026-02-04T16:04:41.254074Z","shell.execute_reply.started":"2026-02-04T16:04:40.673975Z","shell.execute_reply":"2026-02-04T16:04:41.253495Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Step 12: Test the Model","metadata":{}},{"cell_type":"code","source":"print(\"Testing model...\\n\")\n\ndef test_model(user_input, emotion=\"neutral\"):\n    prompt = (\n        \"<|system|>: You are Freud, a calm, empathetic therapeutic AI assistant. \"\n        \"You respond thoughtfully, kindly, and supportively. \"\n        \"You ask gentle follow-up questions and never judge the user.\\n\"\n        f\"<|user|>:\\n\"\n        f\"[emotion: {emotion}]\\n\"\n        f\"{user_input}\\n\"\n        f\"<|assistant|>:\\n\"\n    )\n    \n    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n    \n    with torch.no_grad():\n        outputs = model.generate(\n            **inputs,\n            max_new_tokens=150,\n            temperature=0.7,\n            top_p=0.9,\n            do_sample=True,\n            repetition_penalty=1.2,\n            pad_token_id=tokenizer.pad_token_id,\n            eos_token_id=tokenizer.eos_token_id,\n        )\n    \n    full = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    \n    if \"<|assistant|>:\" in full:\n        response = full.split(\"<|assistant|>:\")[-1].strip()\n        if \"<|user|>\" in response:\n            response = response.split(\"<|user|>\")[0].strip()\n    else:\n        response = full.strip()\n    \n    return response\n\n# Test cases\ntests = [\n    (\"Hi\", \"greeting\"),\n    (\"I feel sad today\", \"sad\"),\n    (\"I'm anxious about my exam\", \"anxious\"),\n    (\"I had a great day!\", \"happy\"),\n]\n\nprint(\"=\"*80)\nfor user, emotion in tests:\n    print(f\"\\nðŸ‘¤ User ({emotion}): {user}\")\n    resp = test_model(user, emotion)\n    print(f\"ðŸ¤– Freud: {resp}\")\n    print(\"-\"*80)\n\nprint(\"\\nTesting complete!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-04T16:04:59.915377Z","iopub.execute_input":"2026-02-04T16:04:59.915670Z","iopub.status.idle":"2026-02-04T16:05:25.272496Z","shell.execute_reply.started":"2026-02-04T16:04:59.915645Z","shell.execute_reply":"2026-02-04T16:05:25.271831Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Step 13: Merge LoRA Adapter ","metadata":{}},{"cell_type":"code","source":"print(\"Merging LoRA adapter...\\n\")\n\n# Reload base\nbase = AutoModelForCausalLM.from_pretrained(\n    BASE_MODEL,\n    device_map=\"auto\",\n    trust_remote_code=True,\n)\n\n# Load and merge\nmerged = PeftModel.from_pretrained(base, OUTPUT_DIR)\nmerged = merged.merge_and_unload()\n\n# Save\nMERGED_DIR = f\"{OUTPUT_DIR}_merged\"\nmerged.save_pretrained(MERGED_DIR)\ntokenizer.save_pretrained(MERGED_DIR)\n\nprint(f\"Merged model: {MERGED_DIR}/\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-04T16:05:56.045012Z","iopub.execute_input":"2026-02-04T16:05:56.045717Z","iopub.status.idle":"2026-02-04T16:06:04.407330Z","shell.execute_reply.started":"2026-02-04T16:05:56.045687Z","shell.execute_reply":"2026-02-04T16:06:04.406287Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import shutil\nshutil.make_archive('freud_model_new_phi', 'zip', '/kaggle/working/freud_phi2_finetuned')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-04T16:25:44.863810Z","iopub.execute_input":"2026-02-04T16:25:44.864640Z","iopub.status.idle":"2026-02-04T16:25:53.971365Z","shell.execute_reply.started":"2026-02-04T16:25:44.864607Z","shell.execute_reply":"2026-02-04T16:25:53.970796Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!zip -r freud_model_1.zip /kaggle/working/third_sem_project/freud_model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-04T16:27:11.913337Z","iopub.execute_input":"2026-02-04T16:27:11.913611Z","iopub.status.idle":"2026-02-04T16:27:12.358233Z","shell.execute_reply.started":"2026-02-04T16:27:11.913586Z","shell.execute_reply":"2026-02-04T16:27:12.357516Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from IPython.display import FileLink\n\nFileLink(r'freud_model_new_phi.zip')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-04T16:28:54.537850Z","iopub.execute_input":"2026-02-04T16:28:54.538459Z","iopub.status.idle":"2026-02-04T16:28:54.543608Z","shell.execute_reply.started":"2026-02-04T16:28:54.538424Z","shell.execute_reply":"2026-02-04T16:28:54.543078Z"}},"outputs":[],"execution_count":null}]}